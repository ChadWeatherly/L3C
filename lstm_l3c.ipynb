{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02a1b4f-c2e9-4448-b8aa-8dc1d9937403",
   "metadata": {},
   "source": [
    "# Code for R3C Exam\n",
    "\n",
    "### Chad Weatherly\n",
    "---\n",
    "\n",
    "The purpose of this notebook is to have a testing ground for code locally before using it in the N3C Enclave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8112cf9-c0f7-408f-83be-1d35b000168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "# These modules need to be installed. The requirements.txt file should take care of all that\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import session_info\n",
    "from scipy.stats import sem\n",
    "from functions import *\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9550b655-e6c1-4caa-ab0c-02c9b4cee0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "matplotlib          3.4.3\n",
      "numpy               1.20.3\n",
      "pandas              1.3.4\n",
      "scipy               1.7.1\n",
      "session_info        1.0.0\n",
      "sklearn             1.0.1\n",
      "torch               1.10.2\n",
      "-----\n",
      "IPython             7.27.0\n",
      "jupyter_client      7.0.1\n",
      "jupyter_core        4.8.1\n",
      "jupyterlab          3.2.1\n",
      "notebook            6.4.5\n",
      "-----\n",
      "Python 3.8.8 (default, Apr 13 2021, 12:59:45) [Clang 10.0.0 ]\n",
      "macOS-10.16-x86_64-i386-64bit\n",
      "-----\n",
      "Session information updated at 2022-10-20 14:58\n"
     ]
    }
   ],
   "source": [
    "# Info for current Jupyter session modules\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "555880c5-6af5-43ed-9304-03e9e206002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 50, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data set\n",
    "\n",
    "x = torch.rand(2000, 50, 1)\n",
    "y = torch.rand(2000, 1)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43afcf20-3f54-4203-a7c6-27ab4732c676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting data ready, splitting and turning into tensors\n",
    "rs = 22\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=rs)\n",
    "splits = kf.split(x, y)\n",
    "\n",
    "k_fold = []\n",
    "for s in splits:\n",
    "    # print(len(s[0]), len(s[1]))\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = x[s[0]], x[s[1]], y[s[0]], y[s[1]]\n",
    "\n",
    "#     x_train.shape\n",
    "\n",
    "    x_train_tn = []\n",
    "    for i in range(len(x_train)):\n",
    "        x_train_tn.append((x_train[i], y_train[i]))\n",
    "    x_train_tn\n",
    "\n",
    "    x_test_tn = []\n",
    "    for i in range(len(x_test)):\n",
    "        x_test_tn.append((x_test[i], y_test[i]))\n",
    "    x_test_tn\n",
    "\n",
    "    # Creating our DataLoaders, which are iterable objects\n",
    "    batch_size = 35\n",
    "    x_train = DataLoader(x_train_tn, batch_size=batch_size)\n",
    "    x_test = DataLoader(x_test_tn, batch_size=batch_size)\n",
    "    \n",
    "    k_fold.append({'x_train':x_train, 'x_test':x_test})\n",
    "    \n",
    "len(k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c05990-0b18-4289-bdd9-d5d8c3a4a8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_train': <torch.utils.data.dataloader.DataLoader at 0x7f967046c4f0>,\n",
       " 'x_test': <torch.utils.data.dataloader.DataLoader at 0x7f9672e51730>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold[0]\n",
    "# Each of the 5 K-folds are PyTorch dataloader objects, iterable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf9e9967-43ae-4fae-91b9-84c690ceee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 50])\n",
      "torch.Size([35])\n"
     ]
    }
   ],
   "source": [
    "for x_i, y_i in k_fold[0]['x_train']:\n",
    "    print(x_i.shape)\n",
    "    print(y_i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f29f3a03-2fad-469f-b58a-f066aa25dea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 8 required positional arguments: 'input_size', 'seq_length', 'batch_size', 'num_lstm_layers', 'dropout', 'num_lin_layers', 'num_lin_units', and 'activation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yf/jvz3gqp91zxdqnsxdmhm157w0000gn/T/ipykernel_1268/2719549343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Showing the structure of the Neural Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mtemp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 8 required positional arguments: 'input_size', 'seq_length', 'batch_size', 'num_lstm_layers', 'dropout', 'num_lin_layers', 'num_lin_units', and 'activation'"
     ]
    }
   ],
   "source": [
    "class lstm(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is the base for a binary classification LSTM model,\n",
    "    which outputs a single value in the interval (0, 1). Assumes batch_first == TRUE\n",
    "    for nn.LSTM\n",
    "    INPUT:\n",
    "        - input_size, the number of features each item in the input sequence has\n",
    "                    (so if the input is a sequence of length L, how many features does\n",
    "                    each item in L have?)\n",
    "        - seq_length, a length of the sequence given\n",
    "        - batch_size, the size of the batch used\n",
    "        - hidden_size, the number of features in the hidden state of the LSTM\n",
    "        - num_lstm_layers, the number of stacked LSTM layers to have\n",
    "        - dropout, the proportion to use for dropout to prevent overfitting\n",
    "        - num_lin_layers, the number of linear, fully-connected layers to have\n",
    "                        following the LSTM layers\n",
    "        - num_lin_units, the number of neurons/units to have in each of the above-mentioned\n",
    "                        linear layers\n",
    "        - activation, the type of activation function to use between linear layers. \n",
    "                    Must be one of these strings:\n",
    "                    - 'elu' = Exponential Linear Unit\n",
    "                    - 'relu' = Rectified Linear Unit\n",
    "                    - 'sigmoid' = Sigmoid function\n",
    "                    - 'tanh' = Hyperbolic Tangent function\n",
    "    OUTPUT:\n",
    "        - Prediction, a value in the interval (0, 1), corresponding to the \n",
    "                    two classes, coded as 0 and 1\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, seq_length, batch_size,\n",
    "                        hidden_size, num_lstm_layers, dropout,\n",
    "                        num_lin_layers, num_lin_units, activation):\n",
    "        # Super inherits all the attributes and methods\n",
    "        # from the Neural Network __init__ method\n",
    "        super(lstm, self).__init__()\n",
    "        # So, now our Neural Network class inherits\n",
    "        # all of the init attributes/methods from\n",
    "        # the inherited class, nn.module\n",
    "        \n",
    "        # Essentially, our Neural Network class is the same as the nn.module\n",
    "        # and then we are adding additional functionality to customize\n",
    "        # it as something that we want\n",
    "        \n",
    "        # Submodules like layers or nn.Sequential are listed as attributes of\n",
    "        # the overall nn.module class (in this case Neural Network)\n",
    "        \n",
    "        # Flatten takes each tensor and \"flattens\" it to create a 1-D vector\n",
    "        # while ignoring batch, and it is used with nn.Sequential\n",
    "        # So a tensor of size (32, 1, 25, 25) would be flattened to (32, 625)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        if activation == 'elu':\n",
    "            self.act = nn.ELU()\n",
    "        elif activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.act = nn.Tanh()\n",
    "        else:\n",
    "            self.act = nn.Sigmoid()\n",
    "        \n",
    "        # Doing LSTM layer first\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                           batch_first=True, dropout=dropout,\n",
    "                           num_layers=num_lstm_layers)\n",
    "        \n",
    "        # Adding first layer to modules\n",
    "        modules = []\n",
    "        if num_lin_layers == 1:\n",
    "            modules.append(nn.Linear(hidden_size*seq_length, 1))\n",
    "            modules.append(nn.Sigmoid())\n",
    "        elif num_lin_layers > 1:\n",
    "            for layer in range(1, num_lin_layers+1):\n",
    "                if layer == 1:\n",
    "                    modules.append(nn.Linear(hidden_size*seq_length, num_lin_units))\n",
    "                    modules.append(self.act)\n",
    "                elif layer == num_lin_layers:\n",
    "                    modules.append(nn.Linear(num_lin_units, 1))\n",
    "                    modules.append(nn.Sigmoid())\n",
    "                else:\n",
    "                    modules.append(nn.Linear(num_lin_units, num_lin_units))\n",
    "                    modules.append(self.act)\n",
    "\n",
    "        # nn.Sequential is a container that holds the layers and chains them together for us\n",
    "        self.linear_stack = nn.Sequential(*modules)\n",
    "            # Modules held inside are submodules of nn.Sequential\n",
    "            # nn.Sequential is treated as one submodule of Neural Network\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden==None:\n",
    "            self.hidden = (torch.zeros(self.num_lstm_layers,\n",
    "                                        x.size()[0], \n",
    "                                        self.hidden_size).float().to(self.device),\n",
    "                           torch.zeros(self.num_lstm_layers,\n",
    "                                        x.size()[0], \n",
    "                                        self.hidden_size).float().to(self.device))\n",
    "        else:\n",
    "            self.hidden = hidden.to(self.device)\n",
    "        \n",
    "        out, self.hidden = self.rnn(x, self.hidden)\n",
    "       \n",
    "        out = self.flatten(out)\n",
    "        logits = self.linear_stack(out)\n",
    "        \n",
    "        return logits, self.hidden\n",
    "    \n",
    "# Showing the structure of the Neural Network\n",
    "temp_model = lstm(hidden_size=25).float().to(device)\n",
    "print(temp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf08b6-4f07-46f5-bc9e-b43137707ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "k_train_loss = []\n",
    "k_test_loss = []\n",
    "k_train_acc = []\n",
    "k_test_acc = []\n",
    "\n",
    "for k in range(len(k_fold)):\n",
    "    print(f'K Fold: {k+1}')    \n",
    "    \n",
    "    model = lstm(hidden_size=25).float().to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    # loss_fn = nn.BCELoss(reduction='mean')\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.0005, momentum=0.2)\n",
    "    # optimizer = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "    \n",
    "    x_train = k_fold[k]['x_train']\n",
    "    x_test = k_fold[k]['x_test']\n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        all_loss = 0\n",
    "        \n",
    "        num_right = 0\n",
    "        size = 0\n",
    "        for i, (x_i, y_i) in enumerate(x_train):\n",
    "            n, m = x_i.shape\n",
    "            x_i = x_i.to(device).reshape(n, 1, m).type(torch.float)\n",
    "            # print(x_i.shape)\n",
    "            y_i = y_i.to(device).type(torch.float).reshape(y_i.shape[0], 1, 1)\n",
    "            y_hat, _ = model(x_i, None)\n",
    "            y_hat = y_hat[:,0].reshape(y_hat.shape[0], 1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            # print(y_hat.shape, y_i.shape)\n",
    "            loss = loss_fn(y_hat, y_i).float()\n",
    "            loss = loss.type(torch.float)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Getting training loss and accuracy\n",
    "            all_loss += loss.item()\n",
    "            \n",
    "            # Getting Training Accuracy\n",
    "            size += n\n",
    "            y_hat_new = y_hat.clone().cpu().detach()\n",
    "            num_right += (y_i.cpu() == y_hat_new.round()).sum().item()\n",
    "        \n",
    "        # If you want average loss, uncomment this line below\n",
    "        # all_loss = all_loss / (len(x_train.dataset))\n",
    "        \n",
    "        # Average loss across each epoch\n",
    "        train_loss.append(all_loss)\n",
    "        train_acc.append(num_right / size)\n",
    "        \n",
    "        if epoch % np.round(epochs/5) == 0 or epoch==epochs:\n",
    "            print(f'\\tepoch:{epoch:4}')\n",
    "            print(f'\\t\\tTraining loss: {all_loss:10.2f}')\n",
    "            print(f'\\t\\tTraining Acc: {(num_right / size):10.2f}')\n",
    "    \n",
    "    \n",
    "        # Test Loss\n",
    "        model.eval()\n",
    "        \n",
    "        num_right = 0\n",
    "        size = 0\n",
    "        with torch.no_grad():\n",
    "            all_loss = 0\n",
    "            for i, (x_i, y_i) in enumerate(x_test):\n",
    "                n, m = x_i.shape\n",
    "                x_i = x_i.to(device).reshape(n, 1, m).type(torch.float)\n",
    "                # print(x_i.shape)\n",
    "                y_i = y_i.to(device).type(torch.float).reshape(y_i.shape[0], 1, 1)\n",
    "                y_hat, _ = model(x_i, None)\n",
    "                y_hat = y_hat[:,0].reshape(y_hat.shape[0], 1, 1)\n",
    "                loss = loss_fn(y_hat, y_i).float()\n",
    "                loss = loss.type(torch.float)\n",
    "                \n",
    "\n",
    "                # Getting loss and accuracy\n",
    "                all_loss += loss.item()\n",
    "                \n",
    "                # Getting Training Accuracy\n",
    "                size += n\n",
    "                y_hat_new = y_hat.clone().cpu().detach()\n",
    "                num_right += (y_i.cpu() == y_hat_new.round()).sum().item()\n",
    "                \n",
    "            # If you want average loss, uncomment this line below\n",
    "            # all_loss = all_loss / (len(x_test.dataset))\n",
    "\n",
    "            # Average loss and accuracy across each epoch\n",
    "            test_loss.append(all_loss)\n",
    "            test_acc.append(num_right / size)\n",
    "            \n",
    "        if epoch % np.round(epochs/5) == 0 or epoch==epochs:\n",
    "            print(f'\\t\\tTesting loss: {all_loss:10.2f}')\n",
    "            print(f'\\t\\tTesting Acc: {(num_right / size):10.2f}')\n",
    "                \n",
    "    k_train_loss.append(train_loss)\n",
    "    k_test_loss.append(test_loss)\n",
    "    k_train_acc.append(train_acc)\n",
    "    k_test_acc.append(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e695ac-217a-4a26-b121-03e4f9a79c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['dimgray', 'lightcoral', 'khaki', 'royalblue', 'seagreen',\n",
    "         'sandybrown', 'palegreen', 'darkorchid', 'maroon', 'aqua']\n",
    "\n",
    "# Showing how the training/testing accuracy changes over epochs\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training/Testing Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "epochs = np.arange(len(k_train_acc[0]))\n",
    "avg_train_acc = []\n",
    "avg_test_acc = []\n",
    "\n",
    "for a in range(len(k_train_acc[0])):\n",
    "    \n",
    "    train_total = 0\n",
    "    test_total = 0\n",
    "    w = len(k_train_acc)\n",
    "    for k in range(w):  \n",
    "        train_total += k_train_acc[k][a]\n",
    "        test_total += k_test_acc[k][a]\n",
    "        \n",
    "    avg_train_acc.append(train_total / w)\n",
    "    avg_test_acc.append(test_total / w)\n",
    "    \n",
    "plt.plot(epochs, avg_train_acc, color=colors[3], label='Training Accuracy', lw=3)\n",
    "plt.plot(epochs, avg_test_acc, color=colors[8], label='Testing Accuracy', lw=3)\n",
    "\n",
    "max_train = avg_train_acc.index(max(avg_train_acc))\n",
    "max_test = avg_test_acc.index(max(avg_test_acc))\n",
    "\n",
    "plt.scatter(epochs[max_train], avg_train_acc[max_train], color='maroon', zorder=2)\n",
    "plt.text(epochs[max_train], avg_train_acc[max_train]+0.03,\n",
    "         f'Max  = {round(avg_train_acc[max_train], 3)}',\n",
    "        horizontalalignment='right')\n",
    "\n",
    "plt.scatter(epochs[max_test], avg_test_acc[max_test], color='b', zorder=2)\n",
    "plt.text(epochs[max_test], avg_test_acc[max_test]-0.06,\n",
    "         f'Max  = {round(avg_test_acc[max_test], 3)}',\n",
    "        horizontalalignment='center')\n",
    "    \n",
    "plt.title('Average Accuracy over Epochs')\n",
    "plt.legend(loc=(1.04,0))\n",
    "\n",
    "# plt.savefig('charts/teacher_training_loss.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d125be-21dd-4d7b-873b-739c360492ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['dimgray', 'lightcoral', 'khaki', 'royalblue', 'seagreen',\n",
    "         'sandybrown', 'palegreen', 'darkorchid', 'maroon', 'aqua']\n",
    "\n",
    "# Showing how the training loss changes over epochs\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "epochs = np.arange(len(k_train_loss[0]))\n",
    "for k in range(len(k_train_loss)):\n",
    "    \n",
    "    plt.plot(epochs, k_train_loss[k], color=colors[k], label='K' + str(k+1), lw=3)\n",
    "    \n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend(loc=(1.04,0))\n",
    "\n",
    "# plt.savefig('charts/teacher_training_loss.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92a6d3-7990-4182-95c2-8c8da7c09ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['dimgray', 'lightcoral', 'khaki', 'royalblue', 'seagreen',\n",
    "         'sandybrown', 'palegreen', 'darkorchid', 'maroon', 'aqua']\n",
    "\n",
    "# Showing how the Testing loss changes over epochs\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Testing Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "epochs = np.arange(len(k_test_loss[0]))\n",
    "for k in range(len(k_test_loss)):\n",
    "    \n",
    "    plt.plot(epochs, k_test_loss[k], color=colors[k], label='K' + str(k+1), lw=3)\n",
    "    \n",
    "plt.title('Testing Loss over Epochs')\n",
    "plt.legend(loc=(1.04,0))\n",
    "\n",
    "# plt.savefig('charts/teacher_testing_loss.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5df4cf-2aa7-4863-84f1-c37c930a413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0\n",
    "test_loss = 0\n",
    "all_train_loss = []\n",
    "all_test_loss = []\n",
    "\n",
    "best_loss = 10000\n",
    "\n",
    "for k in range(len(k_fold)):\n",
    "    print(f'K-Fold {k+1}')\n",
    "    # Final Training Loss\n",
    "    final_train_loss = round(k_train_loss[k][-1], 2)\n",
    "    all_train_loss.append(final_train_loss)\n",
    "    print(f'\\tTraining Loss: {final_train_loss}')\n",
    "    train_loss += final_train_loss\n",
    "    # Final Testing Loss\n",
    "    final_test_loss = round(k_test_loss[k][-1], 2)\n",
    "    all_test_loss.append(final_test_loss)\n",
    "    print(f'\\t Testing Loss: {final_test_loss}')\n",
    "    test_loss += final_test_loss\n",
    "    \n",
    "    avg_loss = (final_train_loss + final_test_loss) / 2 \n",
    "    if avg_loss < best_loss:\n",
    "        best_k = k\n",
    "        best_loss = avg_loss\n",
    "    \n",
    "print()\n",
    "print('----------------------------------')\n",
    "print(f'Average Training Loss: {round(train_loss / len(k_train_loss), 2)}')\n",
    "print(f' Average Testing Loss: {round(test_loss / len(k_train_loss), 2)}')\n",
    "print()\n",
    "print(f'Standard Error of Mean Training Loss: {sem(all_train_loss)}')\n",
    "print(f'Standard Error of Mean Testing Loss: {sem(all_test_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e50599-18ef-4841-8e32-ab632c887491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab1ea2-095a-4e66-abc3-57bcb48a6d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fff2c8-0df3-4b04-a14f-a8b4e9db4167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18897d8f-263f-4547-8b93-ffb9fce411df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8aa199-f177-43d3-81cc-3c2a200c9659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
